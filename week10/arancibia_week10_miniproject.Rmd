---
title: "Week 10 Mini Project Clustering"
author: "Ben Arancibia"
date: "10/31/2015"
output: pdf_document
---

This project will focus on taking data from the R package cluster.datasets nd performing a cluster analysis on the data. The first thing to do is setup the appropriate environment using the following.

```{r,echo=FALSE,warning=FALSE, results='hide',message=FALSE}


Sys.setenv(JAVA_HOME="/usr/lib/jvm/default-java")
Sys.setenv(HADOOP_CMD="/home/bcarancibia/workspace/cuny_msda_is622/hadoop-2.7.1/bin/hadoop")
Sys.setenv(HADOOP_STREAMING="/home/bcarancibia/workspace/cuny_msda_is622/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar")

Sys.setenv(SPARK_HOME = "/home/bcarancibia/workspace/cuny_msda_is622/spark-1.4.1-bin-hadoop2.6")
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))
library(SparkR)
sc <- sparkR.init(master="local")
sqlContext <- sparkRSQL.init(sc)

```


I am going to cluster waiting time between eruptions and the duration of eruptions for the Old Faithful geyser in Yellowstone. This dataset faithful is part of base R.

```{r}

data <- faithful

kmeans.data<- kmeans(data, 3)

plot(data)
points(kmeans.data$centers, pch=19, col="red")


```

Looking at the initial datasets there are two clusters.

```{r}
kmeans.data<- kmeans(data, 2)

plot(data)
points(kmeans.data$centers, pch=19, col="red")

```


Next do this in Spark. I did this using Python in terminal on the dataset. I used the same commands from the Spark documentation and it worked for me. Below are the following commands. 

```
from pyspark.mllib.clustering import KMeans, KMeansModel
from numpy import array
from math import sqrt

# Load and parse the data
data = sc.textFile("data/mllib/faithful.csv")
parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))

# Build the model (cluster the data)
clusters = KMeans.train(parsedData, 2, maxIterations=10,
        runs=10, initializationMode="random")

# Evaluate clustering by computing Within Set Sum of Squared Errors
def error(point):
    center = clusters.centers[clusters.predict(point)]
    return sqrt(sum([x**2 for x in (point - center)]))

WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)
print("Within Set Sum of Squared Error = " + str(WSSSE))

# Save and load model
clusters.save(sc, "myModelPath")
sameModel = KMeansModel.load(sc, "myModelPath")

```

The clustering is the same as using Kmeans in R. See the plot below which is similar to the plot above.

```{r, echo=FALSE}
plot(data)
points(kmeans.data$centers, pch=19, col="red")


```

